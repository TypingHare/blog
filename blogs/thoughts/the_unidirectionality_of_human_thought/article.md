# The Unidirectionality of Human Thought

I have been contemplating an interesting problem: how do we teach artificial intelligence the concept of natural numbers—a foundation idea in mathematics? This question can be simplified into two parts: (a) how do we enable an AI to understand the successor of a given number, and (b) how can it grasp the concept of a number's predecessor?

The challenge lies in abstracting the notions of "successor" and "predecessor." Humans, of course, can effortlessly recognize that the successor of 5 is six, and its predecessor is 4. In this case, each query involves two inputs: a number and a command, either "successor" or "predecessor." While this is straightforward to program, it's a deeper puzzle to consider how we might guide AI in learning these concepts if it lacks the basic mathematical understanding of counting. It raised a question for us as well: did we fully understand "successor" and "predecessor" when we first learned to count? The answer is likely no unless one was exceptionally gifted or retained memories from the previous life.

This suggests that learning should begin with a simpler and more intuitive approach—one that a young child might experience in their first lesson. Recently, I considered an idea: what if our brains store information in a one-way structure so that we only naturally think of "the next" rather than "the previous"? In such a model, numbers might be stored like a singly linked list:
$$
1 \to 2 \to 3 \to \cdots \to 9
$$
where each number leads sequentially to the next. Yet we can still recall the number before a given number, can't we? Let me explain this with a couple of examples.

Take the alphabet. Without starting from the beginning, can you immediately tell the letter that follows "U"? Most can—it is "V." But identifying the preceding letter without reciting the sequence backward is often hard, as we don't typically memorize the alphabet in reverse. If you can do so without having memorized the alphabet backward, it may be because you picture the sequence visually, almost as if relying on a mental image. This involves another brain functionality of our brain, which works in a completely different way.

Consider also the case of memorizing poetry. In high school, my classmates and I often tested each other on ancient poems, and we noticed an interesting pattern: recalling the "next line" was consistently easier than recalling the "previous line." If asked for a preceding line, we often began at the poem's start, working through until we reached the target line. Occasionally, we couldn't recall the poem's title, converting the question into a new one: "Which poem contains this line?" 

These examples show that, generally, human memory retrieval isn’t naturally suited to work backward. Of course, this remains indirect evidence, but we see direct support for this in our neural structures. When a neuron is activated, the electrical impulse travels down its axon, releasing neurotransmitters across the synapse to activate the next connected neuron’s dendrites. This process is strictly one-directional; neurotransmitters cannot pass backward across the synapse to the previous neuron.

Returning to why we can identify a digit’s predecessor so easily, it’s largely because of familiarity. Using decimal numbers daily has solidified associations between them in our minds. If you were learning to count in a new language, recognizing the previous number would likely be challenging, and you might count up from zero each time to pinpoint the number you need.

Furthermore, straight-A students who have completed countless arithmetic exercises can often recall answers like $25-9$, or even harder, $16 \times 9$ almost instantly. This fluency results from creating internal sequences like $25 \to (-) \to 9 \to 16$, and $16 \to (\times) \to 9 \to 144$, which already becomes part of their memory. 

There remains, however, a concept that still puzzles me. Artificial neural networks (ANN) employ backpropagation, a technique where weight matrices are updated backward using algorithms like gradient descent. This differs fundamentally from the human brain, as biological neurons don't communicate back to prior neurons through the same synapse, as mentioned earlier. But that topic is best left for another time, as it lies outside the primary focus here.`

---

After discovering that human thinks unidirectionally, we can conjecture and draft possible explanations for some phenomena.

When learning and analyzing this world, people always pair and connect events by saying one is the cause of another, which is the effect. The cause-effect system is rooted in everyone's mind, and oftentimes, it helps me to speculate many things, enabling us to avoid dangers and risks in our lives. Later, we learn to build logical chains by letting an effect be the cause of another causation, which sometimes ends up in the known avalanche effect.

But the avalanche effect is already a niche, and there is a more important thing we need to ponder behind this. A logical chain is just like the ordered natural numbers or an alphabet, people can easily move from a cause to an effect, but harder to move backward. This is often referred to as "cognitive inertia." Therefore, when listening to a piece of information, we can effortlessly tell the effects based on it, but we need to take some time to think about the cause, which consumes more brain energy, making us more reluctant to do so. If there suddenly appears a person who tells you the cause to the effect, we tend to accept their plausible idea, which seems to take a lot of time to deduct but is generally unreliable.

So, how do we overcome this? Recall the way we learn to count natural numbers backward——we simply memorize a reversal sequence. Similarly, we can intentionally strengthen our memory by keeping reversal logic chains by heart. We automatically do this when we contemplate deeply on a tranquil night, listening to relaxing music, and with our eyes closed. And sometimes, we also need to tidy up our minds by recollecting and reorganizing the clutter causations. As Confucius said: "By reviewing what you have learned and learning new things from it, you can become a teacher. (温故而知新，可以为师矣。)"

I came up about this because the other day, I was debating with my friend on a social issue. A country denoted as C was exposed to a piece of news about using oil tankers to transport cooking oil. My friend believed that the inadequacy of government regulation led to the astonishing event. I agreed with him on this part, but we split on the next part: "The relevant regulatory authorities take taxpayers' money but do nothing."

I then asked him how he could draw this conclusion, and his logic was: "If they did conduct spot checks carefully, then this event could not have happened." It seems to be logically held, but the premise is not sufficient in the real world. I then asked him: "Did you check out how many spot checks they conducted last year? Those reports are open to the public." As the saying goes: "Good news travels slowly, bad news has wings." The public barely pays attention to the positive news, but they let scandals go viral. For a big country with a large population, having some scandals on food safety is common in terms of probability.

Even so, I also didn't have enough evidence to tell that relevant regulatory authorities generally did a great job, but their reputation was suddenly destroyed by such an event. But what I want to point out with this example is that when considering the causes of an effect, we should think of more possibilities and keep them in mind for future use. If we accept others' derivation results without doubt and thinking, both our emotions and reasoning will be led around by the nose.
